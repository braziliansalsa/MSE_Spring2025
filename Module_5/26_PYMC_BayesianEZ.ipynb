{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SergeiVKalinin/MSE_Spring2025/blob/main/Module_5/26_PYMC_BayesianEZ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyF6nIyEspQl"
      },
      "source": [
        "The notebook for the University of Tennessee, Knoxville, Department of Materials Science and Engineering Spring 2025 course, MSE510.\n",
        "- Instructor Sergei V. Kalinin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hpIMs_FdaOH"
      },
      "source": [
        "- These examples are based on the book \"Bayesian Analysis with Python\" by Oswaldo Martin, https://subscription.packtpub.com/book/data/9781805127161/pref"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKF5dPQsvL0d"
      },
      "source": [
        "The convenient library for the visualization of the Markov Chain Monte Carlo (MCMC) results is Arviz, https://python.arviz.org/en/stable/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fW3nhj8629am"
      },
      "outputs": [],
      "source": [
        "!pip install arviz\n",
        "\n",
        "import arviz as az\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zieS58g7LSex"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm, binom, beta, bernoulli, t, multivariate_normal, poisson, randint, entropy, laplace, logistic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1FWOLdONA9h"
      },
      "source": [
        "## Visualizing distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2CI9kGVzMkC"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "print(beta.rvs(5, 11, size=10))\n",
        "\n",
        "az.plot_posterior(beta.rvs(5, 11, size=30000))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cYOZWxCw8uT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY98CF1ANG5T"
      },
      "source": [
        "## First Bayesian model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2Kq1VoRAo0s"
      },
      "source": [
        "There are multiple packages for Bayesian analysis. The relatively simple one is PYMC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr8DW7ii-QD3"
      },
      "outputs": [],
      "source": [
        "import pymc as pm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5khxKT0EVLJ"
      },
      "source": [
        "Let's first simply fit a data to a model - meaning determine the parameters of the distribution from which the data has been drawn. However, note that it doesn't have to be the same distribution - this is a **hypothesis** we are making!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9knYR5s97yf"
      },
      "outputs": [],
      "source": [
        "np.random.seed(123)\n",
        "trials = 30\n",
        "theta_real = 0.35 # unknown value in a real experiment\n",
        "data = bernoulli.rvs(p=theta_real, size=trials)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eliN8-3-M9O"
      },
      "outputs": [],
      "source": [
        "with pm.Model() as our_first_model:\n",
        "    θ = pm.Beta('θ', alpha=1., beta=1.)\n",
        "    y = pm.Bernoulli('y', p=θ, observed=data)\n",
        "    trace = pm.sample(1000, random_seed=123, chains = 3)\n",
        "\n",
        "az.plot_trace(trace)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OW1xKQfE5kD"
      },
      "source": [
        "Let's look at diagnostics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9KuAc1eswwU"
      },
      "outputs": [],
      "source": [
        "az.summary(trace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs0qvkA3Bndo"
      },
      "outputs": [],
      "source": [
        "az.plot_posterior(trace, rope = [0.3, 0.4], ref_val = 0.35)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_OgxWzC1-Ox"
      },
      "source": [
        "We can also use our posteriors to predict some more complex loss functions. For example, it can be cost and time for experiment, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIgj1gY1s77s"
      },
      "outputs": [],
      "source": [
        "grid = np.linspace(0, 1, 200)\n",
        "θ_pos = trace.posterior['θ']\n",
        "lossf_a = [np.mean(abs(i - θ_pos)) for i in grid]\n",
        "lossf_b = [np.mean((i - θ_pos)**2) for i in grid]\n",
        "\n",
        "for lossf, c in zip([lossf_a, lossf_b], ['C0', 'C1']):\n",
        "    mini = np.argmin(lossf)\n",
        "    plt.plot(grid, lossf, c)\n",
        "    plt.plot(grid[mini], lossf[mini], 'o', color=c)\n",
        "    plt.annotate('{:.2f}'.format(grid[mini]),\n",
        "                 (grid[mini], lossf[mini] + 0.03), color=c)\n",
        "    plt.yticks([])\n",
        "    plt.xlabel(r'$\\hat \\theta$')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8ij9piT2rTP"
      },
      "source": [
        "This fucntion can be very complicated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0oxi9eutFFj"
      },
      "outputs": [],
      "source": [
        "lossf = []\n",
        "for i in grid:\n",
        "    if i < 0.5:\n",
        "        f = 1/np.mean(np.pi * θ_pos / np.abs(i - θ_pos))\n",
        "    else:\n",
        "        f = np.mean(1 / (i - θ_pos))\n",
        "    lossf.append(f)\n",
        "\n",
        "mini = np.argmin(lossf)\n",
        "plt.plot(grid, lossf)\n",
        "plt.plot(grid[mini], lossf[mini], 'o')\n",
        "plt.annotate('{:.2f}'.format(grid[mini]),\n",
        "             (grid[mini] + 0.01, lossf[mini] + 0.1))\n",
        "plt.yticks([])\n",
        "plt.xlabel(r'$\\hat \\theta$')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOAaQxbKtQxc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hGZJOmKuXp9"
      },
      "source": [
        "## Fitting data to distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RfzlhP6FMB3"
      },
      "source": [
        "Now, let's do it one more time for a Gaussian model. Here, we have several parameters (mead and dispersion)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-VMlJ7t_UDi"
      },
      "outputs": [],
      "source": [
        "#arr = np.array([0, 0.1, 0.2, 0.3, 0.4, 0.8, 0.16, 0.32, 0.50, 0.150])\n",
        "\n",
        "μ, σ = 0., 0.2\n",
        "X = norm(μ, σ)\n",
        "arr = X.rvs(300)\n",
        "\n",
        "az.plot_posterior(arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXcDFtH-th8L"
      },
      "outputs": [],
      "source": [
        "with pm.Model() as model_g:\n",
        "    μ = pm.Uniform('μ', lower=-1, upper=2)\n",
        "    σ = pm.HalfNormal('σ', sigma=1)\n",
        "    y = pm.Normal('y', mu=μ, sigma=σ, observed=arr)\n",
        "    idata_g = pm.sample(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyqfi6lzts99"
      },
      "outputs": [],
      "source": [
        "az.plot_trace(idata_g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX7sN-dEtv2U"
      },
      "outputs": [],
      "source": [
        "az.plot_pair(idata_g, kind='kde')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9-G0q2FtyIM"
      },
      "outputs": [],
      "source": [
        "az.summary(idata_g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwwCHsXpt1JF"
      },
      "outputs": [],
      "source": [
        "pm.sample_posterior_predictive(idata_g, model=model_g, extend_inferencedata=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "916ix0TGt5Id"
      },
      "outputs": [],
      "source": [
        "ax = az.plot_ppc(idata_g, num_pp_samples=100, figsize=(12, 6), mean=False)\n",
        "plt.hist(arr, bins = 50, color = 'r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4Gzm28Mt_Ut"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u71eYp2pvkhj"
      },
      "source": [
        "## Linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT1B60NBFVeA"
      },
      "source": [
        "Now, let's explore how we can use Bayesian methods to fit data with the linear function. First, we make and plot some data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94qullChvoHL"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "N = 100\n",
        "alpha_real = 2.5\n",
        "beta_real = 0.9\n",
        "eps_real = np.random.normal(0, 0.5, size=N)\n",
        "\n",
        "x = np.random.normal(10, 1, N)\n",
        "y_real = alpha_real + beta_real * x\n",
        "y = y_real + eps_real"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHFauD_BvzOV"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "ax[0].plot(x, y, 'C0.')\n",
        "ax[0].set_xlabel('x')\n",
        "ax[0].set_ylabel('y', rotation=0)\n",
        "ax[0].plot(x, y_real, 'k')\n",
        "az.plot_kde(y, ax=ax[1])\n",
        "ax[1].set_xlabel('y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_Qwp9NpFgGA"
      },
      "source": [
        "Next, we define the probabilistic model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGO9a6KHv1Od"
      },
      "outputs": [],
      "source": [
        "with pm.Model() as model_g:\n",
        "    α = pm.Normal('α', mu=0, sigma=10)\n",
        "    β = pm.Normal('β', mu=0, sigma=1)\n",
        "    ϵ = pm.HalfCauchy('ϵ', 5)\n",
        "\n",
        "    μ = pm.Deterministic('μ', α + β * x)\n",
        "    y_pred = pm.Normal('y_pred', mu=μ, sigma=ϵ, observed=y)\n",
        "\n",
        "    idata_g = pm.sample(2000, tune=2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCoK1CHFFk4R"
      },
      "source": [
        "And voila! Note that in the code below we **draw** pairs of parameters alpha and beta (i.e. offset and slope) and plot the lines corresponding to each pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afyESom9v3n9"
      },
      "outputs": [],
      "source": [
        "plt.plot(x, y, 'C0.')\n",
        "\n",
        "posterior_g = az.extract(idata_g)\n",
        "alpha_m = posterior_g['α'].mean().item()\n",
        "beta_m = posterior_g['β'].mean().item()\n",
        "\n",
        "draws = range(0, posterior_g.sample.size, 10)\n",
        "plt.plot(x, posterior_g['α'][draws].values + posterior_g['β'][draws].values * x[:,None], c='gray', alpha=0.5)\n",
        "\n",
        "plt.plot(x, alpha_m + beta_m * x, c='k',\n",
        "         label=f'y = {alpha_m:.2f} + {beta_m:.2f} * x')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y', rotation=0)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYMCR-NvFxs_"
      },
      "source": [
        "We can look at the same summary. Note that now our model calculated the distirbution fo each point in the parameter space (if you were wondering where the high dimensional parameter spaces are coming from)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHiVUt5pdNVi"
      },
      "outputs": [],
      "source": [
        "az.summary(idata_g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4miIGhY8GE1f"
      },
      "source": [
        "Let's look at joint distribution. To make things easy, we just pull the pairs of (slope, offset) as individual traces, and analyze them via Seaborn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGG6efj8icrZ"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbMeQ04ddRxZ"
      },
      "outputs": [],
      "source": [
        "sns.jointplot(x = posterior_g['α'][draws].values, y = posterior_g['β'][draws].values,kind='kde')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ-TbpmNGQW9"
      },
      "source": [
        "Now, we can calculate the highest density interval (HDI) for the predicted behaviors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddJgV65Dv_H1"
      },
      "outputs": [],
      "source": [
        "plt.plot(x, alpha_m + beta_m * x, c='k',\n",
        "         label=f'y = {alpha_m:.2f} + {beta_m:.2f} * x')\n",
        "\n",
        "sig = az.plot_hdi(x, posterior_g['μ'].T, hdi_prob=0.99, color='k')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y', rotation=0)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwRSbIOTGeoH"
      },
      "source": [
        "We can also run the posterior predictive check to see whether the data is consistent with the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vc7YFapIwJqd"
      },
      "outputs": [],
      "source": [
        "ppc = pm.sample_posterior_predictive(idata_g, model=model_g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSsrpxZFwJxu"
      },
      "outputs": [],
      "source": [
        "plt.plot(x, y, 'b.')\n",
        "plt.plot(x, alpha_m + beta_m * x, c='k',\n",
        "         label=f'y = {alpha_m:.2f} + {beta_m:.2f} * x')\n",
        "\n",
        "az.plot_hdi(x, ppc.posterior_predictive['y_pred'], hdi_prob=0.5, color='gray')\n",
        "az.plot_hdi(x, ppc.posterior_predictive['y_pred'], color='gray')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y', rotation=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdZTRImJxBe2"
      },
      "source": [
        "## Robust regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kCk2Aa50Mp-"
      },
      "source": [
        "Here, we are going to explore Bayesian linear fit of the data set with the strong outlier. Previously, we have used the RANSAC method for it. Now, let's see what Bayesian linear fit can do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d13jIEcPxEs0"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import scipy.stats as stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMIf9UL4xLUR"
      },
      "outputs": [],
      "source": [
        "ans = sns.load_dataset('anscombe')\n",
        "print(ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNBFMnk2xRKh"
      },
      "outputs": [],
      "source": [
        "x_3 = ans[ans.dataset == 'III']['x'].values\n",
        "y_3 = ans[ans.dataset == 'III']['y'].values\n",
        "x_3 = x_3 - x_3.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-L8Xsuxxh8e"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "beta_c, alpha_c = stats.linregress(x_3, y_3)[:2]\n",
        "ax[0].plot(x_3, (alpha_c + beta_c * x_3), 'k',\n",
        "           label=f'y ={alpha_c:.2f} + {beta_c:.2f} * x')\n",
        "ax[0].plot(x_3, y_3, 'C0o')\n",
        "ax[0].set_xlabel('x')\n",
        "ax[0].set_ylabel('y', rotation=0)\n",
        "ax[0].legend(loc=0)\n",
        "az.plot_kde(y_3, ax=ax[1], rug=True)\n",
        "ax[1].set_xlabel('y')\n",
        "ax[1].set_yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pACtlXOu3fUI"
      },
      "source": [
        "We can introduce distributions that account for outliers - e.g. Student distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LXdmJuY3cAg"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "x_values = np.linspace(-10, 10, 500)\n",
        "for df in [1, 2, 30]:\n",
        "    distri = stats.t(df)\n",
        "    x_pdf = distri.pdf(x_values)\n",
        "    plt.plot(x_values, x_pdf, label=fr'$\\nu = {df}$', lw=3)\n",
        "\n",
        "x_pdf = stats.norm.pdf(x_values)\n",
        "plt.plot(x_values, x_pdf, 'k--', label=r'$\\nu = \\infty$')\n",
        "plt.xlabel('x')\n",
        "plt.yticks([])\n",
        "plt.legend()\n",
        "plt.xlim(-5, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNH0DkzT0dIC"
      },
      "source": [
        "Let's define the model. Note that we have (going from bottom up):\n",
        "- defined our process as Student distribution where mean is the linear function of parameters alpha and beta and there is noise epsilon\n",
        "- the parameter v of student's distribution is given by exponential distribution\n",
        "- noise epsilon has half-Normal distribution\n",
        "- parameters alpha and beta have normal distributions estimated from observed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-I6Yb-rxsNX"
      },
      "outputs": [],
      "source": [
        "with pm.Model() as model_t:\n",
        "    α = pm.Normal('α', mu=y_3.mean(), sigma=1)\n",
        "    β = pm.Normal('β', mu=0, sigma=1)\n",
        "    ϵ = pm.HalfNormal('ϵ', 5)\n",
        "    ν_ = pm.Exponential('ν_', 1/29)\n",
        "    ν = pm.Deterministic('ν', ν_ + 1)\n",
        "\n",
        "    y_pred = pm.StudentT('y_pred', mu=α + β * x_3,\n",
        "                         sigma=ϵ, nu=ν, observed=y_3)\n",
        "\n",
        "    idata_t = pm.sample(2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZd_n1W3xwmG"
      },
      "outputs": [],
      "source": [
        "varnames = ['α', 'β', 'ϵ', 'ν']\n",
        "az.plot_trace(idata_t, var_names=varnames);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TG7zXt8ExzAH"
      },
      "outputs": [],
      "source": [
        "beta_c, alpha_c = stats.linregress(x_3, y_3)[:2]\n",
        "\n",
        "posterior_mean = az.extract(idata_t).mean()\n",
        "\n",
        "plt.plot(x_3, (alpha_c + beta_c * x_3), 'k', label='non-robust', alpha=0.5)\n",
        "plt.plot(x_3, y_3, 'C0o')\n",
        "alpha_m = posterior_mean['α'].item()\n",
        "beta_m = posterior_mean['β'].item()\n",
        "plt.plot(x_3, alpha_m + beta_m * x_3, c='k', label='robust')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y', rotation=0)\n",
        "plt.legend(loc=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPA7tWacx2Qt"
      },
      "outputs": [],
      "source": [
        "az.summary(idata_t, var_names=varnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7OI-ktuxkJe"
      },
      "outputs": [],
      "source": [
        "pm.sample_posterior_predictive(idata_t, model=model_t, random_seed=2, extend_inferencedata=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNwvMXxNx8hu"
      },
      "outputs": [],
      "source": [
        "ax = az.plot_ppc(idata_t, num_pp_samples=200, figsize=(12, 6), mean=True)\n",
        "plt.xlim(0, 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7SOtu9uyOjP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdWnQUukyPRG"
      },
      "source": [
        "## Hierarchical regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uQlmfie1IyZ"
      },
      "source": [
        "It turns out, we can use Bayesian methods to solve problems that are simply impossible by classical methods. For example, can we fit a line to a single point? Turns out we can do it, if we have other data sets and have reasons to believe that they share some common data generation process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7InCva_oxUy-"
      },
      "outputs": [],
      "source": [
        "N = 20\n",
        "M = 8\n",
        "idx = np.repeat(range(M-1), N)\n",
        "idx = np.append(idx, 7)\n",
        "np.random.seed(314)\n",
        "\n",
        "alpha_real = np.random.normal(2.5, 0.5, size=M)\n",
        "beta_real = np.random.beta(6, 1, size=M)\n",
        "eps_real = np.random.normal(0, 0.5, size=len(idx))\n",
        "\n",
        "y_m = np.zeros(len(idx))\n",
        "x_m = np.random.normal(10, 1, len(idx))\n",
        "y_m = alpha_real[idx] + beta_real[idx] * x_m + eps_real\n",
        "\n",
        "_, ax = plt.subplots(2, 4, figsize=(10, 5), sharex=True, sharey=True)\n",
        "ax = np.ravel(ax)\n",
        "j, k = 0, N\n",
        "for i in range(M):\n",
        "    ax[i].scatter(x_m[j:k], y_m[j:k])\n",
        "    ax[i].set_xlabel(f'x_{i}')\n",
        "    ax[i].set_ylabel(f'y_{i}', rotation=0, labelpad=15)\n",
        "    ax[i].set_xlim(6, 15)\n",
        "    ax[i].set_ylim(7, 17)\n",
        "    j += N\n",
        "    k += N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pLBPF9mycud"
      },
      "outputs": [],
      "source": [
        "x_centered = x_m - x_m.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYoGHO8R1jke"
      },
      "source": [
        "Let's fit this data independently"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh_q_pCRyhFl"
      },
      "outputs": [],
      "source": [
        "with pm.Model() as unpooled_model:\n",
        "    α_tmp = pm.Normal('α_tmp', mu=0, sigma=10, shape=M)\n",
        "    β = pm.Normal('β', mu=0, sigma=10, shape=M)\n",
        "    ϵ = pm.HalfCauchy('ϵ', 5)\n",
        "    ν = pm.Exponential('ν', 1/30)\n",
        "\n",
        "    y_pred = pm.StudentT('y_pred', mu=α_tmp[idx] + β[idx] * x_centered,\n",
        "                         sigma=ϵ, nu=ν, observed=y_m)\n",
        "\n",
        "    α = pm.Deterministic('α', α_tmp - β * x_m.mean())\n",
        "\n",
        "    idata_up = pm.sample(2000, target_accept=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuDN2gsQyjyO"
      },
      "outputs": [],
      "source": [
        "az.plot_forest(idata_up, var_names=['α', 'β'], combined=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWCeHsDP1njC"
      },
      "source": [
        "And now let's make hierarchical model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jnj9kGoiym4V"
      },
      "outputs": [],
      "source": [
        "with pm.Model() as hierarchical_model:\n",
        "    # hyper-priors\n",
        "    α_μ_tmp = pm.Normal('α_μ_tmp', mu=0, sigma=10)\n",
        "    α_σ_tmp = pm.HalfNormal('α_σ_tmp', 10)\n",
        "    β_μ = pm.Normal('β_μ', mu=0, sigma=10)\n",
        "    β_σ = pm.HalfNormal('β_σ', sigma=10)\n",
        "\n",
        "    # priors\n",
        "    α_tmp = pm.Normal('α_tmp', mu=α_μ_tmp, sigma=α_σ_tmp, shape=M)\n",
        "    β = pm.Normal('β', mu=β_μ, sigma=β_σ, shape=M)\n",
        "    ϵ = pm.HalfCauchy('ϵ', 5)\n",
        "    ν = pm.Exponential('ν', 1/30)\n",
        "\n",
        "    y_pred = pm.StudentT('y_pred', mu=α_tmp[idx] + β[idx] * x_centered,\n",
        "                         sigma=ϵ, nu=ν, observed=y_m)\n",
        "\n",
        "    α = pm.Deterministic('α', α_tmp - β * x_m.mean())\n",
        "    α_μ = pm.Deterministic('α_μ', α_μ_tmp - β_μ * x_m.mean())\n",
        "    α_σ = pm.Deterministic('α_sd', α_σ_tmp - β_μ * x_m.mean())\n",
        "\n",
        "    idata_hm = pm.sample(2000, target_accept=0.99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25_aC4-EyvK2"
      },
      "outputs": [],
      "source": [
        "az.plot_forest(idata_hm, var_names=['α', 'β'], combined=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqmKRaqYywDW"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(2, 4, figsize=(10, 5), sharex=True, sharey=True)\n",
        "ax = np.ravel(ax)\n",
        "j, k = 0, N\n",
        "x_range = np.linspace(x_m.min(), x_m.max(), 10)\n",
        "\n",
        "posterior_hm = az.extract(idata_hm)\n",
        "\n",
        "for i in range(M):\n",
        "    ax[i].scatter(x_m[j:k], y_m[j:k])\n",
        "    ax[i].set_xlabel(f'x_{i}')\n",
        "    ax[i].set_ylabel(f'y_{i}', labelpad=17, rotation=0)\n",
        "    alpha_m = posterior_hm[\"α\"].sel({\"α_dim_0\":i}).mean().item()\n",
        "    beta_m = posterior_hm[\"β\"].sel({\"β_dim_0\":i}).mean().item()\n",
        "    ax[i].plot(x_range, alpha_m + beta_m * x_range, c='k',\n",
        "               label=f'y = {alpha_m:.2f} + {beta_m:.2f} * x')\n",
        "    plt.xlim(x_m.min()-1, x_m.max()+1)\n",
        "    plt.ylim(y_m.min()-1, y_m.max()+1)\n",
        "    j += N\n",
        "    k += N"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNP5ohf3zn7yfLuAG6jKgQJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}